# NeurIPS 2024 Reading

list of neurips 2024 papers 

* https://papercopilot.com/paper-list/neurips-paper-list/neurips-2024-paper-list/

### Selected Papers 


1. Random Representations Outperform Online Continually Learned Representations [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/10)]
2. BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages (Alice Oh) [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/11)]
3. Mission Impossible: A Statistical Perspective on Jailbreaking LLMs [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/12)]
4. Iteration Head: A Mechanistic Study of Chain-of-Thought [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/15)]
5. The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/16)]
6. SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
7. On Affine Homotopy between Language Encoders
8. Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents
9. A Theoretical Understanding of Self-Correction through In-context Alignment
10. Kaleido Diffusion - Improving Conditional Diffusion Models with Autoregressive Latent Modeling
11. Exploring and Improving Drafts in Blockwise Parallel Decoding
12. CVQA:Culturally-diverse Multilingual Visual Question Answering Benchmark
13. Scaling Retrieval-Based Langauge Models with a Trillion-Token Datastore
14. WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs
15. Spectral Editing of Activations for Large Language Model Alignment
16. Interpreting the Weight Space of Customized Diffusion Models
17. Scaling transformer neural networks for skillful and reliable medium-range weather forecasting
18. End-To-End Causal Effect Estimation from Unstructured Natural Language Data [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/13)]
19. HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/3)] (ðŸ’« spotlight)
20. Precise Relational DNN Verification With Cross Executional Branching
21. Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/4)]
22. ZeroMark: Towards Dataset Ownership Verification without Disclosing Dataset-specified Watermarks
23. Not All Tokens Are What You Need for Pretraining [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/5)] (ðŸ’« oral)
24. A Taxonomy of Challenges to Curating Fair Datasets [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/6)] (ðŸ’« oral)
25. Do Finetti: On Causal Effects for Exchangeable Data [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/7)] (ðŸ’« oral)
26. Guiding a Diffusion Model with a Bad Version of Itself [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/8)]  (ðŸ’« oral)
27. Achieving Efficient Alignment through Learned Correction (ðŸ’« oral)
28. LLM Evaluators Recognize and Favor Their Own Generations (ðŸ’« oral)
29. Stylus: Automatic Adapter Selection for Diffusion Models (ðŸ’« oral)
30. Questioning the Survey Responses of Large Language Models (ðŸ’« oral)
31. Statistical Efficiency of Distributional Temporal Differencev (ðŸ’« oral)
32. CAT3D: Create Anything in 3D with Multi-View Diffusion Models (ðŸ’« oral)
33. RL-GPT: Integrating Reinforcement Learning and Code-as-policy (ðŸ’« oral)
34. Enhancing Preference-based Linear Bandits via Human Response Time (ðŸ’« oral)
35. OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset (ðŸ’« oral)
36. The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models (ðŸ’« oral)
37. Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought (ðŸ’« oral)
38. Understanding, Rehearsing, and Introspecting: Learn a Policy from Textual Tutorial Books in Football Games (ðŸ’« oral)
39. LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages (ðŸ’« oral)
40. Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes (ðŸ’« oral)
41. Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks (ðŸ’« oral)
42. Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators (ðŸ’« oral)
43. ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction (ðŸ’« oral)
44. RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation (ðŸ’« oral)
45. Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures (ðŸ’« oral)
46. Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs (ðŸ’« oral)
47. Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery (ðŸ’« oral)
48. 3702	Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity (ðŸ’« oral)
49. Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models (ðŸ’« oral)
50. Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering (ðŸ’« oral)
51. Improving Environment Novelty Quantification for Effective Unsupervised Environment Design (ðŸ’« oral)
52. Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation  (ðŸ’« oral)
53. DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices (ðŸ’« oral) 
54. Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli (ðŸ’« oral) 
55. Learning rigid-body simulators over implicit shapes for large-scale scenes and vision  (ðŸ’« oral) 
56. Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments  (ðŸ’« oral)
57. DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs  (ðŸ’« oral)
58. DenoiseReID: Denoising Model for Representation Learning of Person Re-Identification  (ðŸ’« oral)
59. Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction  (ðŸ’« oral)
60. Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning  (ðŸ’« oral)
61. Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure  (ðŸ’« oral)
62. Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework  (ðŸ’« oral)
63. Gaussian-Informed Continuum for Physical Property Identification and Simulation  (ðŸ’« oral)
64. MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model  (ðŸ’« oral)
65. PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression  (ðŸ’« oral)
66. NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction  (ðŸ’« oral)
67. CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark  (ðŸ’« oral)
68. MedCalc-Bench: Evaluating Large Language Models for Medical Calculations  (ðŸ’« oral)
69. Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making  (ðŸ’« oral)
70. Solving Intricate Problems with Human-like Decomposition and Rethinking  (ðŸ’« oral)
71. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision Making  (ðŸ’« oral) 
72. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs  (ðŸ’« oral) 
73. You Only Cache Once: Decoder-Decoder Architectures for Language Models  (ðŸ’« oral)
74. The Sample-Communication Complexity Trade-off in Federated Q-Learning  (ðŸ’« oral) 
75. SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling  (ðŸ’« oral)
76. MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map  (ðŸ’« oral) 
77. Improved Distribution Matching Distillation for Fast Image Synthesis  (ðŸ’« oral)
78. HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning  (ðŸ’« oral)
79. DevBench: A multimodal developmental benchmark for language learning  (ðŸ’« oral)
80. Neural Pfaffians: Solving Many Many-Electron SchrÃ¶dinger Equations  (ðŸ’« oral)
81. E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection  (ðŸ’« oral)
82. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents  (ðŸ’« oral)
83. VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time  (ðŸ’« oral)
84. MoEUT: Mixture-of-Experts Universal Transformers
85. Mixture of Demonstrations for In-Context Learning
86. Linear Transformers are Versatile In-Context Learners
87. Retrieval & Fine-Tuning for In-Context Tabular Models
88. Learnable In-Context Vector for Visual Question Answering
89. A Simple Image Segmentation Framework via In-Context Examples
90. Hybrid Mamba: An Promising In-Context RL for Long-Term Decision
91. Mixture of In-Context Experts Enhance LLMs' Long Context Awareness
92. On the Noise Robustness of In-Context Learning for Text Generation
93. Where does In-context Learning \\ Happen in Large Language Models?
94. Linking In-context Learning in Transformers to Human Episodic Memory
95. In-Context Learning State Vector with Inner and Momentum Optimization
96. Universal In-Context Approximation By Prompting Fully Recurrent Models
97. Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning
98. BERTs are Generative In-Context Learners
99. Opponent Modeling with In-context Search
100. Can large language models explore in-context?
101. Mixture of Demonstrations for In-Context Learning
102. Analysing the Generalisation and Reliability of Steering Vectors
103. Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization
104. In-Context Learning with Representations: Contextual Generalization of Trained Transformers
105. Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models
106. A StrongREJECT for Empty Jailbreaks
107. Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
108. Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
109. Fight Back Against Jailbreaking via Prompt Adversarial Tuning
110. When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
111. Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization
112. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models
113. Unified Covariate Adjustment for Causal Inference
114. Disentangled Representation Learning in Non-Markovian Causal Systems
115. Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making
116. Causal Imitation for Markov Decision Processes: a Partial Identification Approach [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/14)]
117. Partial Transportability for Domain Generalization
118. What Factors Affect Multi-modal In-Context Learning? An In-Depth Exploration
119. Algorithmic Capabilities of Random Transformers [[Discussion(https://github.com/room1805/NeurIPS2024/discussions/18)]]
120. Interpreting CLIP with Sparse Linear Concept Embeddings (SpLiCE)
121. On Causal Discovery in the Presence of Deterministic Relations
122. Mind the Gap: A Causal Perspective on Bias Amplification in Prediction & Decision-Making
123. Transformer Efficiently Learns Low-dimensional Target Functions In-context
124. Opponent Modeling with In-context Search
125. Can large language models explore in-context?
126. Symmetries In-Context: Universal Self-Supervised Learning through Contextual World Models
127. The Closeness of In-Context Learning and Weight Shifting for Softmax Regression
128. How In-Context Learning Emerges from Training on Unstructured Data: The Role of Co-Occurrence, Positional Information, and Noise Structures
129. Drift-Resilient TabPFN: In-Context Learning Distribution Shifts on Tabular Data
130. Enhancing In-Context Learning Performance with just SVD-Based Weight Pruning: A Theoretical Perspective
131. In-Context Learning of a Linear Transformer Block: Benefits of the MLP Component and One-Step GD Initialization
132. In-Context Learning with Representations: Contextual Generalization of Trained Transformers
133. BERTs are Generative In-Context Learners
134. DETAIL: Task DEmonsTration Attribution for Interpretable In-context Learning
135. Universal In-Context Approximation By Prompting Fully Recurrent Models
136. Transformers as Game Players: Provable In-context Game-playing Capabilities of Pre-trained Models
137. Fine-grained Analysis of In-context Linear Estimation
138. Transformers are Minimax Optimal Nonparametric In-Context Learners
139. Linear Transformers are Versatile In-Context Learners
140. Towards Understanding How Transformers Learn In-context Through a Representation Learning Lens
141. Many-Shot In-Context Learning
142. Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning
143. On the Noise Robustness of In-Context Learning for Text Generation
144. How Transformers Utilize Multi-Head Attention in In-Context Learning? A Case Study on Sparse Linear Regression
145. LG-CAV: Train Any Concept Activation Vector with Language Guidance
146. Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders
147. DreamSteerer: Enhancing Source Image Conditioned Editability using Personalized Diffusion Models





