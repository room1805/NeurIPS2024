# NeurIPS 2024 Reading

list of neurips 2024 papers 

* https://papercopilot.com/paper-list/neurips-paper-list/neurips-2024-paper-list/

### Selected Papers 


1. Random Representations Outperform Online Continually Learned Representations [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/10)]
2. BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages (Alice Oh) [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/11)]
3. Mission Impossible: A Statistical Perspective on Jailbreaking LLMs [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/12)]
4. Iteration Head: A Mechanistic Study of Chain-of-Thought
5. The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains
6. SELMA: Learning and Merging Skill-Specific Text-to-Image Experts with Auto-Generated Data
7. On Affine Homotopy between Language Encoders
8. Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents
9. A Theoretical Understanding of Self-Correction through In-context Alignment
10. Kaleido Diffusion - Improving Conditional Diffusion Models with Autoregressive Latent Modeling
11. Exploring and Improving Drafts in Blockwise Parallel Decoding
12. CVQA:Culturally-diverse Multilingual Visual Question Answering Benchmark
13. Scaling Retrieval-Based Langauge Models with a Trillion-Token Datastore
14. WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs
15. Spectral Editing of Activations for Large Language Model Alignment
16. Interpreting the Weight Space of Customized Diffusion Models
17. Scaling transformer neural networks for skillful and reliable medium-range weather forecasting
18. End-To-End Causal Effect Estimation from Unstructured Natural Language Data [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/13)]
19. HaloScope: Harnessing Unlabeled LLM Generations for Hallucination Detection [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/3)] (ðŸ’« spotlight)
20. Precise Relational DNN Verification With Cross Executional Branching
21. Scalable Neural Network Verification with Branch-and-bound Inferred Cutting Planes [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/4)]
22. ZeroMark: Towards Dataset Ownership Verification without Disclosing Dataset-specified Watermarks
23. Not All Tokens Are What You Need for Pretraining [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/5)] (ðŸ’« oral)
24. A Taxonomy of Challenges to Curating Fair Datasets [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/6)] (ðŸ’« oral)
25. Do Finetti: On Causal Effects for Exchangeable Data [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/7)] (ðŸ’« oral)
26. Guiding a Diffusion Model with a Bad Version of Itself [[Discussion](https://github.com/room1805/NeurIPS2024/discussions/8)]  (ðŸ’« oral)
27. Achieving Efficient Alignment through Learned Correction (ðŸ’« oral)
28. LLM Evaluators Recognize and Favor Their Own Generations (ðŸ’« oral)
29. Stylus: Automatic Adapter Selection for Diffusion Models (ðŸ’« oral)
30. Questioning the Survey Responses of Large Language Models (ðŸ’« oral)
31. Statistical Efficiency of Distributional Temporal Differencev (ðŸ’« oral)
32. CAT3D: Create Anything in 3D with Multi-View Diffusion Models (ðŸ’« oral)
33. RL-GPT: Integrating Reinforcement Learning and Code-as-policy (ðŸ’« oral)
34. Enhancing Preference-based Linear Bandits via Human Response Time (ðŸ’« oral)
35. OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset (ðŸ’« oral)
36. The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models (ðŸ’« oral)
37. Unlocking the Boundaries of Thought: A Reasoning Granularity Framework to Quantify and Optimize Chain-of-Thought (ðŸ’« oral)
38. Understanding, Rehearsing, and Introspecting: Learn a Policy from Textual Tutorial Books in Football Games (ðŸ’« oral)
39. LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages (ðŸ’« oral)
40. Trading Place for Space: Increasing Location Resolution Reduces Contextual Capacity in Hippocampal Codes (ðŸ’« oral)
41. Learning to grok: Emergence of in-context learning and skill composition in modular arithmetic tasks (ðŸ’« oral)
42. Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators (ðŸ’« oral)
43. ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction (ðŸ’« oral)
44. RG-SAN: Rule-Guided Spatial Awareness Network for End-to-End 3D Referring Expression Segmentation (ðŸ’« oral)
45. Achieving Optimal Clustering in Gaussian Mixture Models with Anisotropic Covariance Structures (ðŸ’« oral)
46. Span-Based Optimal Sample Complexity for Weakly Communicating and General Average Reward MDPs (ðŸ’« oral)
47. Flipped Classroom: Aligning Teacher Attention with Student in Generalized Category Discovery (ðŸ’« oral)
48. 3702	Reinforcement Learning Under Latent Dynamics: Toward Statistical and Algorithmic Modularity (ðŸ’« oral)
49. Maximum Entropy Inverse Reinforcement Learning of Diffusion Models with Energy-Based Models (ðŸ’« oral)
50. Exploitation of a Latent Mechanism in Graph Contrastive Learning: Representation Scattering (ðŸ’« oral)
51. Improving Environment Novelty Quantification for Effective Unsupervised Environment Design (ðŸ’« oral)
52. Divide-and-Conquer Meets Consensus: Unleashing the Power of Functions in Code Generation  (ðŸ’« oral)
53. DapperFL: Domain Adaptive Federated Learning with Model Fusion Pruning for Edge Devices (ðŸ’« oral) 
54. Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli (ðŸ’« oral) 
55. Learning rigid-body simulators over implicit shapes for large-scale scenes and vision  (ðŸ’« oral) 
56. Identification and Estimation of the Bi-Directional MR with Some Invalid Instruments  (ðŸ’« oral)
57. DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs  (ðŸ’« oral)
58. DenoiseReID: Denoising Model for Representation Learning of Person Re-Identification  (ðŸ’« oral)
59. Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction  (ðŸ’« oral)
60. Weisfeiler and Leman Go Loopy: A New Hierarchy for Graph Representational Learning  (ðŸ’« oral)
61. Generalization Error Bounds for Two-stage Recommender Systems with Tree Structure  (ðŸ’« oral)
62. Get Rid of Isolation: A Continuous Multi-task Spatio-Temporal Learning Framework  (ðŸ’« oral)
63. Gaussian-Informed Continuum for Physical Property Identification and Simulation  (ðŸ’« oral)
64. MeshFormer : High-Quality Mesh Generation with 3D-Guided Reconstruction Model  (ðŸ’« oral)
65. PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression  (ðŸ’« oral)
66. NeuroClips: Towards High-fidelity and Smooth fMRI-to-Video Reconstruction  (ðŸ’« oral)
67. CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark  (ðŸ’« oral)
68. MedCalc-Bench: Evaluating Large Language Models for Medical Calculations  (ðŸ’« oral)
69. Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making  (ðŸ’« oral)
70. Solving Intricate Problems with Human-like Decomposition and Rethinking  (ðŸ’« oral)
71. MDAgents: An Adaptive Collaboration of LLMs for Medical Decision Making  (ðŸ’« oral) 
72. Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs  (ðŸ’« oral) 
73. You Only Cache Once: Decoder-Decoder Architectures for Language Models  (ðŸ’« oral)
74. The Sample-Communication Complexity Trade-off in Federated Q-Learning  (ðŸ’« oral) 
75. SeeA*: Efficient Exploration-Enhanced A* Search by Selective Sampling  (ðŸ’« oral)
76. MetaLA: Unified Optimal Linear Approximation to Softmax Attention Map  (ðŸ’« oral) 
77. Improved Distribution Matching Distillation for Fast Image Synthesis  (ðŸ’« oral)
78. HydraLoRA: An Asymmetric LoRA Architecture for Efficient Fine-Tuning  (ðŸ’« oral)
79. DevBench: A multimodal developmental benchmark for language learning  (ðŸ’« oral)
80. Neural Pfaffians: Solving Many Many-Electron SchrÃ¶dinger Equations  (ðŸ’« oral)
81. E2E-MFD: Towards End-to-End Synchronous Multimodal Fusion Detection  (ðŸ’« oral)
82. AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents  (ðŸ’« oral)
83. VASA-1: Lifelike Audio-Driven Talking Faces Generated in Real Time  (ðŸ’« oral)
84. MoEUT: Mixture-of-Experts Universal Transformers
85. Mixture of Demonstrations for In-Context Learning
86. Linear Transformers are Versatile In-Context Learners
87. Retrieval & Fine-Tuning for In-Context Tabular Models
88. Learnable In-Context Vector for Visual Question Answering
89. A Simple Image Segmentation Framework via In-Context Examples
90. Hybrid Mamba: An Promising In-Context RL for Long-Term Decision
91. Mixture of In-Context Experts Enhance LLMs' Long Context Awareness
92. On the Noise Robustness of In-Context Learning for Text Generation
93. Where does In-context Learning \\ Happen in Large Language Models?
94. Linking In-context Learning in Transformers to Human Episodic Memory
95. In-Context Learning State Vector with Inner and Momentum Optimization
96. Universal In-Context Approximation By Prompting Fully Recurrent Models
97. Multimodal Task Vectors Enable Many-Shot Multimodal In-Context Learning
98. BERTs are Generative In-Context Learners
99. Opponent Modeling with In-context Search
100. Can large language models explore in-context?
101. Mixture of Demonstrations for In-Context Learning
102. Analysing the Generalisation and Reliability of Steering Vectors
103. Personalized Steering of Large Language Models: Versatile Steering Vectors Through Bi-directional Preference Optimization
104. In-Context Learning with Representations: Contextual Generalization of Trained Transformers
105. Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models
106. A StrongREJECT for Empty Jailbreaks
107. Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs
108. Tree of Attacks: Jailbreaking Black-Box LLMs Automatically
109. Fight Back Against Jailbreaking via Prompt Adversarial Tuning
110. When LLM Meets DRL: Advancing Jailbreaking Efficiency via DRL-guided Search
111. Efficient LLM Jailbreak via Adaptive Dense-to-sparse Constrained Optimization
112. JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models






